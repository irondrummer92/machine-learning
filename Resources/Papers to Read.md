### Documenting list of Papers to read:

[Understanding the difficulty of training deep feedforward neural networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
<br/><br/>
<strike>[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)</strike>
<br/><br/>
[Batch Renormalization](https://arxiv.org/pdf/1702.03275.pdf)
<br/><br/>
[Opening the Black Box of Deep Neural Nets](https://arxiv.org/pdf/1703.00810.pdf)
<br/><br/>
<strike>[Dropout Regularization](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)</strike>
<br/><br/>
[Shattered Gradients](https://arxiv.org/pdf/1702.08591.pdf)
<br/><br/>
[Dropout Training as Adaptive Regularization](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)
<br/><br/>
[ADVANCES IN OPTIMIZING RECURRENT NETWORKS](https://arxiv.org/pdf/1212.0901v2.pdf)
<br/><br/>
[ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980.pdf)
<br/><br/>
[Practical Recommendations for Gradient-Based Training of Deep Architectures](https://arxiv.org/pdf/1206.5533v2.pdf)
<br/><br/>
[ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
